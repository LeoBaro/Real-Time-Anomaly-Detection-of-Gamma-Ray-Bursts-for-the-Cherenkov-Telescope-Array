{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "critical-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__: 2.1.3\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"tf.__version__: {tf.__version__}\")\n",
    "if tf.test.gpu_device_name(): \n",
    "    print(f\"Default GPU Device:{tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "from os import getcwd\n",
    "import os.path\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-neutral",
   "metadata": {},
   "source": [
    "## Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outdoor-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_generation_for_models.ipynb\t       photometry_lib.ipynb\r\n",
      "lstm_experiment_1.ipynb\t\t\t       simple_autoencoder.ipynb\r\n",
      "lstm_experiment_2.ipynb\t\t\t       training_conv\r\n",
      "models\t\t\t\t\t       training_lstm\r\n",
      "notebook_dataset_generation_for_models_output  training_lstm2\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outer-cartoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/baroncelli/phd/rtapipe/analysis/notebook_dataset_generation_for_models_output\n"
     ]
    }
   ],
   "source": [
    "datapath = Path(\"/home/baroncelli/phd/rtapipe/analysis/notebook_dataset_generation_for_models_output\")\n",
    "print(datapath)\n",
    "assert datapath.is_dir() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "corresponding-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/baroncelli/phd/rtapipe/analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentdir = getcwd()\n",
    "currentdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demographic-tragedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/baroncelli/phd/rtapipe/analysis/notebook_lstm_output')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outdir = Path(currentdir).joinpath(\"notebook_lstm_output\")\n",
    "outdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "suffering-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator:\n",
    "    \n",
    "    def __init__(self, input_width, label_width, shift):\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "\n",
    "    def loadClassDataFromDir(dataDir, sampleClass, sampleSize, windowSize, deleteRows=0):\n",
    "        \n",
    "        dataframes = [  pd.read_csv(str(dataDir.joinpath(f)), header=0) for f in listdir(dataDir) if dataDir.joinpath(f).is_file()]\n",
    "        \n",
    "        dataframe = pd.concat(dataframes, axis=0)\n",
    "\n",
    "        if deleteRows > 0:\n",
    "            dataframe = dataframe[:-deleteRows]\n",
    "\n",
    "        numberOfSamples = int(dataframe.shape[0]/sampleSize)\n",
    "        bkgTimespan = round( (dataframe.shape[0]*windowSize)/60/60, 2)\n",
    "\n",
    "        print(f\"Found {len(dataframes)} files! Each file has: \\n\\t{dataframes[0].shape[0]} rows (samples) \\n\\t{dataframes[0].shape[1]} columns!\")\n",
    "\n",
    "        print(f\"Temporal size = {bkgTimespan} h. Each sample is equivalent to {bkgTimespan/numberOfSamples*60*60} seconds\")\n",
    "\n",
    "\n",
    "        ## USE A WINDOW GENERATOR HERE?\n",
    "\n",
    "        dataframe.head()\n",
    "        dataframe = pd.DataFrame(dataframe[\"COUNTS\"].values.reshape(numberOfSamples, sampleSize))\n",
    "        dataframe[sampleSize] = sampleClass\n",
    "        print(dataframe.shape)\n",
    "        print(dataframe.head(2))\n",
    "\n",
    "\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "\n",
    "    def addData(self, label, label_columns=None):\n",
    "\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def split(self):\n",
    "\n",
    "        train_data, other_data, train_labels, other_labels = train_test_split(\n",
    "            data, labels, test_size=0.4, random_state=69\n",
    "        )\n",
    "\n",
    "        test_data, val_data, test_labels, val_labels = train_test_split(\n",
    "            other_data, other_labels, test_size=0.1, random_state=69\n",
    "        )        \n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "    def split_window(self, features):\n",
    "\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                          axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes\n",
    "        # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def plot(self, model=None, plot_col='COUNTS', max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    for n in range(max_n):\n",
    "    plt.subplot(3, 1, n+1)\n",
    "    plt.ylabel(f'{plot_col} [normed]')\n",
    "    plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "          label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "    if self.label_columns:\n",
    "    label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "    else:\n",
    "    label_col_index = plot_col_index\n",
    "\n",
    "    if label_col_index is None:\n",
    "    continue\n",
    "\n",
    "    plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "              edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "    if model is not None:\n",
    "    predictions = model(inputs)\n",
    "    plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                marker='X', edgecolors='k', label='Predictions',\n",
    "                c='#ff7f0e', s=64)\n",
    "\n",
    "    if n == 0:\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [h]')\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=32,)\n",
    "\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "    @property\n",
    "    def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        # No example batch was found, so get one from the `.train` dataset\n",
    "        result = next(iter(self.train))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "requested-hampshire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "refined-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 180\n",
    "WINDOW_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "muslim-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files! Each file has: \n",
      "\t180 rows (samples) \n",
      "\t5 columns!\n",
      "Temporal size = 50.0 h. Each sample is equivalent to 1800.0 seconds\n",
      "(100, 181)\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  171  172  173  174  \\\n",
      "0  293  235  279  224  277  256  272  252  245  260  ...  253  252  279  252   \n",
      "1  251  267  273  288  282  212  283  301  233  255  ...  262  244  267  268   \n",
      "\n",
      "   175  176  177  178  179  180  \n",
      "0  249  244  274  248  239    0  \n",
      "1  251  251  269  270  250    0  \n",
      "\n",
      "[2 rows x 181 columns]\n"
     ]
    }
   ],
   "source": [
    "bkgDataDir = datapath.joinpath(\"run0406_ID000126_OK_bkg_only\",\"csv\")\n",
    "bkgSamples = processDataDir(bkgDataDir, 0, SAMPLE_SIZE, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "tired-orange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 files! Each file has: \n",
      "\t180 rows (samples) \n",
      "\t5 columns!\n",
      "Temporal size = 50.0 h. Each sample is equivalent to 1800.0 seconds\n",
      "(100, 181)\n",
      "   0    1    2    3    4    5    6    7    8    9    ...  171  172  173  174  \\\n",
      "0  243  251  286  257  274  249  284  296  275  267  ...  286  244  256  285   \n",
      "1  273  237  280  262  255  267  236  258  286  268  ...  258  282  250  265   \n",
      "\n",
      "   175  176  177  178  179  180  \n",
      "0  300  285  274  269  262    1  \n",
      "1  246  272  274  284  275    1  \n",
      "\n",
      "[2 rows x 181 columns]\n"
     ]
    }
   ],
   "source": [
    "bkgSrcDataDir = datapath.joinpath(\"run0406_ID000126_OK_bkg_src\",\"csv\")\n",
    "bkgSrcSamples = processDataDir(bkgSrcDataDir, 1, SAMPLE_SIZE, WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = WindowGenerator(input_width=10, label_width=1, shift=10, label_columns=['COUNTS'], val_df=val_df, train_df=train_df, test_df=test_df)\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-lewis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-education",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-impression",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-performer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-detection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appreciated-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Input, Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "possible-catch",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-21bcb713d54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelLSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodelLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodelLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodelLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodelLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "modelLSTM = Sequential()\n",
    "modelLSTM.add(LSTM(32, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "modelLSTM.add(Dropout(rate=0.3))\n",
    "modelLSTM.add(RepeatVector(x_train.shape[1]))\n",
    "modelLSTM.add(LSTM(32, return_sequences=True))\n",
    "modelLSTM.add(Dropout(rate=0.3))\n",
    "modelLSTM.add(TimeDistributed(Dense(x_train.shape[2])))\n",
    "\n",
    "modelLSTM.compile(optimizer='adam', loss='mae')\n",
    "modelLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path_lstm = Path(\"./training_lstm/cp.ckpt\")\n",
    "checkpoint_path_lstm.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback_lstm  = keras.callbacks.ModelCheckpoint(filepath=str(checkpoint_path_lstm), save_weights_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTMHistory = modelLSTM.fit(x_train, x_train, epochs=1, batch_size=128, validation_split=0.1, verbose=1, callbacks=[cp_callback_lstm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-corruption",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-species",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
