{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoBaro/phd/blob/main/rtapipe/analysis/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceramic-celebrity"
      },
      "source": [
        "# Sequence-to-Sequence Prediction Problems\n",
        "Sequence prediction often involves forecasting the next value in a real valued sequence or outputting a class label for an input sequence.\n",
        "\n",
        "This is often framed as a sequence of one input time step to one output time step (e.g. one-to-one) or multiple input time steps to one output time step (many-to-one) type sequence prediction problem.\n",
        "\n",
        "One approach to seq2seq prediction problems that has proven very effective is called the Encoder-Decoder LSTM.\n",
        "\n",
        "## Encoder-Decoder LSTM \n",
        "The LSTM network can be organized into an architecture called the Encoder-Decoder LSTM that allows the model to be used to both support variable length input sequences and to predict or output variable length output sequences.\n",
        "\n",
        "In this architecture, an encoder LSTM model reads the input sequence step-by-step. After reading in the entire input sequence, the hidden state or output of this model represents an internal learned representation of the entire input sequence as a fixed-length vector. This vector is then provided as an input to the decoder model that interprets it as each step in the output sequence is generated\n",
        "This architecture is comprised of two models: one for reading the input sequence and encoding it into a fixed-length vector, and a second for decoding the fixed-length vector and outputting the predicted sequence. The use of the models in concert gives the architecture its name of Encoder-Decoder LSTM designed specifically for seq2seq problems.\n",
        "The innovation of this architecture is the use of a fixed-sized internal representation in the heart of the model that input sequences are read to and output sequences are read from. For this reason, the method may be referred to as sequence embedding.\n",
        "\n",
        "    … RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence.\n",
        "\n",
        "    — Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014.\n",
        "\n",
        "The Encoder-Decoder LSTM was developed for natural language processing problems where it demonstrated state-of-the-art performance, specifically in the area of text translation called statistical machine translation. \n",
        "\n",
        "    The proposed RNN Encoder-Decoder naturally generates a continuous-space representation of a phrase. […] From the visualization, it is clear that the RNN Encoder-Decoder captures both semantic and syntactic structures of the phrases\n",
        "\n",
        "    — Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014."
      ],
      "id": "ceramic-celebrity"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moving-terrace"
      },
      "source": [
        "## Keras implementation\n",
        "\n",
        "For a given dataset of sequences, an encoder-decoder LSTM is configured to read the input sequence, encode it, decode it, and recreate it. The performance of the model is evaluated based on the model’s ability to recreate the input sequence.\n",
        "\n",
        "Once the model achieves a desired level of performance recreating the sequence, the decoder part of the model may be removed, leaving just the encoder model. This model can then be used to encode input sequences to a fixed-length vector.\n",
        "\n",
        "The resulting vectors can then be used in a variety of applications, not least as a compressed representation of the sequence as an input to another supervised learning model.\n",
        "\n",
        "We can think of the model as being comprised of two key parts: the encoder and the decoder.\n",
        "\n",
        "One or more LSTM layers can be used to implement the encoder model. The output of this model is a fixed-size vector that represents the internal representation of the input sequence. The number of memory cells in this layer defines the length of this fixed-sized vector."
      ],
      "id": "moving-terrace"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9IPBWWRAQFs"
      },
      "source": [
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "id": "k9IPBWWRAQFs",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neural-highland"
      },
      "source": [
        "sequence = np.array(list([round(x*0.1,1) for x in range(1, 10, 1)]))"
      ],
      "id": "neural-highland",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opening-young",
        "outputId": "5732e434-c1cc-42f5-8da8-396e7bd2f0a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sequence = sequence.reshape((1, len(sequence), 1))\n",
        "sequence"
      ],
      "id": "opening-young",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.1],\n",
              "        [0.2],\n",
              "        [0.3],\n",
              "        [0.4],\n",
              "        [0.5],\n",
              "        [0.6],\n",
              "        [0.7],\n",
              "        [0.8],\n",
              "        [0.9]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diverse-roulette"
      },
      "source": [
        ""
      ],
      "id": "diverse-roulette",
      "execution_count": null,
      "outputs": []
    }
  ]
}